{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from efficientnet_pytorch) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py): started\n",
      "  Building wheel for efficientnet_pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16522 sha256=b71bc9c8ba1f23088c87891cf50ceaa98bee5179203e83172b56c54a787b1d42\n",
      "  Stored in directory: c:\\users\\fortn\\appdata\\local\\pip\\cache\\wheels\\29\\16\\24\\752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torchmetrics) (1.24.3)\n",
      "Requirement already satisfied: packaging>17.1 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torchmetrics) (2.6.0+cu124)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: setuptools in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.8.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
      "Requirement already satisfied: networkx in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.5)\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "   ---------------------------------------- 0.0/927.3 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/927.3 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 927.3/927.3 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.12.0 torchmetrics-1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.2 MB 8.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/11.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.2 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.2 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.2 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Precision, Recall\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение преобразований для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Изменение размера изображения\n",
    "    transforms.ToTensor(),          # Преобразование в тензор\n",
    "    transforms.Normalize(           # Нормализация\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class SubclassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):  # Изменяем имя параметра на \"root\"\n",
    "        self.root = root  # Используем \"root\" вместо \"root_dir\"\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.subclass_to_idx = {}\n",
    "        self.idx_to_subclass = {}\n",
    "        self.classes = []  # Добавляем атрибут classes\n",
    "\n",
    "        # Рекурсивно проходим по всем папкам и изображениям\n",
    "        for class_name in sorted(os.listdir(self.root)):\n",
    "            class_path = os.path.join(self.root, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "\n",
    "            for subclass_name in sorted(os.listdir(class_path)):\n",
    "                subclass_path = os.path.join(class_path, subclass_name)\n",
    "                if not os.path.isdir(subclass_path):\n",
    "                    continue\n",
    "\n",
    "                # Добавляем подкласс в словарь меток\n",
    "                if subclass_name not in self.subclass_to_idx:\n",
    "                    idx = len(self.subclass_to_idx)\n",
    "                    self.subclass_to_idx[subclass_name] = idx\n",
    "                    self.idx_to_subclass[idx] = subclass_name\n",
    "                    self.classes.append(subclass_name)  # Добавляем подкласс в список classes\n",
    "\n",
    "                # Добавляем все изображения в этот подкласс\n",
    "                for image_name in sorted(os.listdir(subclass_path)):\n",
    "                    image_path = os.path.join(subclass_path, image_name)\n",
    "                    if os.path.isfile(image_path):\n",
    "                        self.samples.append((image_path, self.subclass_to_idx[subclass_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обучающего и тестового наборов данных\n",
    "train_dataset = SubclassDataset(root=\"D:/Dina/LogoLensAI/ml_experements/set/train_and_test/train\", transform=transform)\n",
    "test_dataset =SubclassDataset(root=\"D:/Dina/LogoLensAI/ml_experements/set/train_and_test/test\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Замена последнего слоя для задачи классификации\n",
    "num_classes = len(train_dataset.classes)  # Количество классов в датасете\n",
    "model._fc = nn.Linear(model._fc.in_features, num_classes)\n",
    "\n",
    "# Перемещение модели на GPU (если доступно)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обучения модели\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Обучение на трейне ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # --- Оценка на тесте ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_loader, desc=f'Testing Epoch {epoch+1}/{num_epochs}'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Вычисление метрик\n",
    "        accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                    f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "                    f\"Test Accuracy: {accuracy:.2f}%, \"\n",
    "                    f\"Test Precision: {precision:.4f}, \"\n",
    "                    f\"Test Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/15: 100%|██████████| 914/914 [04:32<00:00,  3.36it/s]\n",
      "Testing Epoch 1/15: 100%|██████████| 393/393 [01:11<00:00,  5.52it/s]\n",
      "\u001b[32m2025-02-04 15:21:02.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [1/15], Train Loss: 4.5373, Test Accuracy: 47.54%, Test Precision: 0.5530, Test Recall: 0.4713\u001b[0m\n",
      "Training Epoch 2/15: 100%|██████████| 914/914 [04:31<00:00,  3.37it/s]\n",
      "Testing Epoch 2/15: 100%|██████████| 393/393 [01:10<00:00,  5.57it/s]\n",
      "\u001b[32m2025-02-04 15:26:44.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [2/15], Train Loss: 2.1011, Test Accuracy: 59.87%, Test Precision: 0.6638, Test Recall: 0.5962\u001b[0m\n",
      "Training Epoch 3/15: 100%|██████████| 914/914 [04:31<00:00,  3.37it/s]\n",
      "Testing Epoch 3/15: 100%|██████████| 393/393 [01:10<00:00,  5.55it/s]\n",
      "\u001b[32m2025-02-04 15:32:26.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [3/15], Train Loss: 1.2793, Test Accuracy: 63.90%, Test Precision: 0.6853, Test Recall: 0.6367\u001b[0m\n",
      "Training Epoch 4/15: 100%|██████████| 914/914 [04:31<00:00,  3.36it/s]\n",
      "Testing Epoch 4/15: 100%|██████████| 393/393 [01:10<00:00,  5.56it/s]\n",
      "\u001b[32m2025-02-04 15:38:08.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [4/15], Train Loss: 0.8377, Test Accuracy: 65.51%, Test Precision: 0.7002, Test Recall: 0.6525\u001b[0m\n",
      "Training Epoch 5/15: 100%|██████████| 914/914 [04:32<00:00,  3.35it/s]\n",
      "Testing Epoch 5/15: 100%|██████████| 393/393 [01:11<00:00,  5.47it/s]\n",
      "\u001b[32m2025-02-04 15:43:53.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [5/15], Train Loss: 0.5733, Test Accuracy: 65.95%, Test Precision: 0.7048, Test Recall: 0.6578\u001b[0m\n",
      "Training Epoch 6/15: 100%|██████████| 914/914 [04:35<00:00,  3.32it/s]\n",
      "Testing Epoch 6/15: 100%|██████████| 393/393 [01:11<00:00,  5.48it/s]\n",
      "\u001b[32m2025-02-04 15:49:40.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [6/15], Train Loss: 0.4184, Test Accuracy: 65.90%, Test Precision: 0.7084, Test Recall: 0.6570\u001b[0m\n",
      "Training Epoch 7/15: 100%|██████████| 914/914 [04:36<00:00,  3.31it/s]\n",
      "Testing Epoch 7/15: 100%|██████████| 393/393 [01:12<00:00,  5.45it/s]\n",
      "\u001b[32m2025-02-04 15:55:28.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [7/15], Train Loss: 0.3362, Test Accuracy: 66.07%, Test Precision: 0.7100, Test Recall: 0.6595\u001b[0m\n",
      "Training Epoch 8/15: 100%|██████████| 914/914 [04:34<00:00,  3.33it/s]\n",
      "Testing Epoch 8/15: 100%|██████████| 393/393 [01:11<00:00,  5.51it/s]\n",
      "\u001b[32m2025-02-04 16:01:14.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [8/15], Train Loss: 0.2811, Test Accuracy: 67.25%, Test Precision: 0.7184, Test Recall: 0.6704\u001b[0m\n",
      "Training Epoch 9/15: 100%|██████████| 914/914 [04:34<00:00,  3.33it/s]\n",
      "Testing Epoch 9/15: 100%|██████████| 393/393 [01:10<00:00,  5.55it/s]\n",
      "\u001b[32m2025-02-04 16:07:00.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [9/15], Train Loss: 0.2535, Test Accuracy: 67.12%, Test Precision: 0.7154, Test Recall: 0.6688\u001b[0m\n",
      "Training Epoch 10/15: 100%|██████████| 914/914 [04:34<00:00,  3.33it/s]\n",
      "Testing Epoch 10/15: 100%|██████████| 393/393 [01:10<00:00,  5.54it/s]\n",
      "\u001b[32m2025-02-04 16:12:45.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [10/15], Train Loss: 0.2314, Test Accuracy: 67.07%, Test Precision: 0.7123, Test Recall: 0.6684\u001b[0m\n",
      "Training Epoch 11/15: 100%|██████████| 914/914 [04:34<00:00,  3.33it/s]\n",
      "Testing Epoch 11/15: 100%|██████████| 393/393 [01:11<00:00,  5.53it/s]\n",
      "\u001b[32m2025-02-04 16:18:31.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [11/15], Train Loss: 0.2106, Test Accuracy: 67.54%, Test Precision: 0.7177, Test Recall: 0.6732\u001b[0m\n",
      "Training Epoch 12/15: 100%|██████████| 914/914 [04:34<00:00,  3.33it/s]\n",
      "Testing Epoch 12/15: 100%|██████████| 393/393 [01:10<00:00,  5.54it/s]\n",
      "\u001b[32m2025-02-04 16:24:16.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [12/15], Train Loss: 0.1958, Test Accuracy: 68.04%, Test Precision: 0.7222, Test Recall: 0.6790\u001b[0m\n",
      "Training Epoch 13/15: 100%|██████████| 914/914 [04:31<00:00,  3.36it/s]\n",
      "Testing Epoch 13/15: 100%|██████████| 393/393 [01:10<00:00,  5.54it/s]\n",
      "\u001b[32m2025-02-04 16:29:59.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [13/15], Train Loss: 0.1848, Test Accuracy: 67.35%, Test Precision: 0.7220, Test Recall: 0.6723\u001b[0m\n",
      "Training Epoch 14/15: 100%|██████████| 914/914 [04:31<00:00,  3.36it/s]\n",
      "Testing Epoch 14/15: 100%|██████████| 393/393 [01:10<00:00,  5.54it/s]\n",
      "\u001b[32m2025-02-04 16:35:42.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [14/15], Train Loss: 0.1809, Test Accuracy: 67.87%, Test Precision: 0.7127, Test Recall: 0.6775\u001b[0m\n",
      "Training Epoch 15/15: 100%|██████████| 914/914 [04:31<00:00,  3.36it/s]\n",
      "Testing Epoch 15/15: 100%|██████████| 393/393 [01:10<00:00,  5.56it/s]\n",
      "\u001b[32m2025-02-04 16:41:25.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mEpoch [15/15], Train Loss: 0.1620, Test Accuracy: 67.39%, Test Precision: 0.7185, Test Recall: 0.6721\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), \"efficientnet_logo2k.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Predicted class: Toyota\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Загрузка модели\n",
    "def load_model(model_path, num_classes, device):\n",
    "    # Создаем модель EfficientNet-B0\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    model._fc = torch.nn.Linear(model._fc.in_features, num_classes)\n",
    "    \n",
    "    # Загружаем веса модели\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Перемещаем модель на устройство (CPU или GPU)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    return model\n",
    "\n",
    "# Функция для инференса\n",
    "def predict_image_class(image_path, model, class_names, device):\n",
    "    # Определяем преобразования для входного изображения\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # EfficientNet-B0 ожидает входное изображение размером 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Стандартные значения нормализации\n",
    "    ])\n",
    "    \n",
    "    # Загружаем изображение\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Применяем преобразования\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Добавляем размерность батча (batch dimension)\n",
    "    \n",
    "    # Перемещаем изображение на устройство (CPU или GPU)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Выполняем предсказание\n",
    "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted_idx = torch.max(outputs, 1)  # Получаем индекс максимального значения\n",
    "    \n",
    "    # Преобразуем индекс в название класса\n",
    "    predicted_class = class_names[predicted_idx.item()]\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "# Параметры\n",
    "model_path = \"D:/Dina/LogoLensAI/ml_experements/efficientnet_logo2k.pth\"  # Путь к файлу с весами модели\n",
    "num_classes = len(train_dataset.classes)\n",
    "class_names =  train_dataset.classes # Список названий классов\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Выбираем устройство\n",
    "\n",
    "# Загружаем модель\n",
    "model = load_model(model_path, num_classes, device)\n",
    "\n",
    "# Путь к изображению\n",
    "image_path = \"D:/Dina/LogoLensAI/ml_experements/data_img/toyota_1.jpg\"\n",
    "\n",
    "# Выполняем предсказание\n",
    "predicted_class = predict_image_class(image_path, model, class_names, device)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\fortn\\appdata\\local\\temp\\pip-req-build-e7vb8y4b\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from clip==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: wcwidth in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: colorama in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\anaconda3\\envs\\dina\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369594 sha256=01a007c5d0825feaeaf9325f0e1318ae3d7820e552a12eeffe37ddeb3b0d0cc3\n",
      "  Stored in directory: C:\\Users\\fortn\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-p4iyq5qy\\wheels\\c8\\e4\\e1\\11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\fortn\\AppData\\Local\\Temp\\pip-req-build-e7vb8y4b'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self, labels: list, device: str = None, clip_model_name: str = \"ViT-B/32\"):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.model, self.preprocess = clip.load(clip_model_name, device=self.device)\n",
    "        \n",
    "        self.labels = labels \n",
    "        self.text_inputs = clip.tokenize(self.labels).to(self.device)\n",
    "\n",
    "    def classify_image(self, image: Image.Image) -> dict:\n",
    "        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits_per_image, _ = self.model(image_input, self.text_inputs)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n",
    "        result = dict(zip(self.labels, probs))\n",
    "        return result\n",
    "    \n",
    "   \n",
    "\n",
    "    def classify_collage(self, image_path: str) -> int:\n",
    "        image = Image.open(image_path)\n",
    "        image_np = np.array(image)\n",
    "\n",
    "\n",
    "        pil_img = Image.fromarray(image_np)\n",
    "        result = self.classify_image(pil_img)\n",
    "        best_key = max(result, key=result.get)\n",
    "        best_value = result[best_key]\n",
    "        \n",
    "        return best_key, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/Dina/LogoLensAI/ml_experements/data_img/VK-1.jpg'\n",
    "labels = [ \"other\", 'is this VK?']\n",
    "classifier = Classifier(labels = labels)\n",
    "key, value = classifier.classify_collage(file_path)\n",
    "print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
